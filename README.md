# forest-dungeon
Выпускная Квалификационная Работа

Непосредственно тема работы - "Обучение с подкреплением, основанное на любопытстве в Action игре" 
(Использовался инструментарий ML-Agents). 

# Задачи
1. Изучить предложенный алгоритм обучения с подкреплением, основанного на любопытстве, с помощью самоконтролируемого прогноза и определить необходимые технологии. 
2. Обучить агентов в тестовой среде, а затем интегрировать модель в прототип Action игры.
3. Сравнить процесс и результаты обучения с любопытством и без.
4. Сделать выводы о целесообразности применения.

# Архитектура. Стек Технологий


![image](https://user-images.githubusercontent.com/55749490/180568826-14894f62-e0aa-419c-b522-d0145a92b0d7.png)

# Структура
1. Сцены (основная и тренировочные)
2. Скрипты
3. Файлы для анимации и отрисовки местности
4. Модели противников


![2022-07-23_00-09-29](https://user-images.githubusercontent.com/55749490/180568996-eae4fa36-ef07-4a3a-8661-0a7c1be9ab92.png)

# Функционал агентов


![Рисунок1](https://user-images.githubusercontent.com/55749490/180569241-8c59d39c-1695-4958-824f-9ca556798679.jpg)


# Система награждения
+15 за столкновение с целью
+30 за ее уничтожение
-5 за столкновение со стенами \ препятствиями


# Демонстрация
На видео представлены противники, корректно выполняющие свой функционал

https://user-images.githubusercontent.com/55749490/180569478-822e6cb5-514b-43e6-b94c-3de11a028cc3.mp4





https://user-images.githubusercontent.com/55749490/180569538-3279c620-479d-4e8a-86cd-0670c17ba83e.mp4

# Результаты сравнения


![Рисунок2](https://user-images.githubusercontent.com/55749490/180570070-e44d621e-64c1-4723-b48f-640ac7a2bf10.png)

![Рисунок3](https://user-images.githubusercontent.com/55749490/180570072-9a39fdee-6d25-43c2-9695-685b4b75bdc2.png)

![Рисунок4](https://user-images.githubusercontent.com/55749490/180570067-ea4361ab-9def-47c4-9a23-2d393366aa06.png)

# Заключение

  В ходе выполнения выпускной работы исследовалось применение обучения с подкреплением, основанного на любопытстве, с помощью самоконтролируемого прогноза в условиях внедрения в прототип Action-RPG игры, с последующим сравнением с обучением без него. Для этого был разработан проект с полноценным игровым уровнем (противники, окружение и т.д.) с отдельными сценами для тренировки обучаемых агентов.
  
  Рассматривались случаи поведения противников в средах, отличающихся от тех, в которых они обучались - как итог все агенты показали хорошие результаты в исследовании местности и принятии решений, исправно выполняя задуманный для них функционал при обучении с предложенным методом. В сравнении с просто обучением с подкреплением, рассматриваемый подход действительно позволяет агентам лучше ориентироваться в среде при редком внешнем вознаграждении от нее.
  
  Были сделаны выводы, что успех обучения зависит от местности, где непосредственно оно проводится, плотности функции вознаграждения агентов, а также объема и определения информации, предоставляемой агенту для выполнения задачи. 
	Но в то же время обучение с любопытством не покажет сильно лучших результатов в условиях широкого спектра условий вознаграждения – область применения скорее в условиях достаточно простых вознаграждений, таких как проигрыш \ выигрыш или выполнение \ провал задания, где агенту нужно действительно обширно изучать его окружение.



